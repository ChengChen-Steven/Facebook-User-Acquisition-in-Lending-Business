{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Marketing Project: Data Analytics Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Project Source: Some Company <br>\n",
    "* Creator: Cheng Chen <br>\n",
    "* Date: 07/07/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of Analytics Report\n",
    "1. Data Cleaning & Exploration <br>\n",
    "    1.1 Configurate the environment & load datasets <br>\n",
    "    1.2 Define some functions <br>\n",
    "    1.3 Clean dataset - dat_repay <br>\n",
    "    1.4 Clean dataset - dat_ppl <br>\n",
    "    1.5 Clean dataset - dat_dev <br>\n",
    "    1.6 Clean dataset - dat_mpesa <br>\n",
    " <br>    \n",
    "2. Modeling the Likelihood of Repayment <br>\n",
    "    2.1 Prepare the combined dataset <br>\n",
    "    2.2 Modeling the likelihood of repayment <br>\n",
    "    2.3 Support the decisions <br>\n",
    "    2.4 Understand the customers through dat_mpesa <br>\n",
    " <br>    \n",
    "3. Modeling the Life-Time-Value (LTV) of Borrowers <br>\n",
    "    3.1 Prepare the dataset: dat_ltv <br>\n",
    "    3.2 Modeling the LTV using logistic regression <br>\n",
    "    3.3 Support the decisions <br>\n",
    "    3.4 Understand the customers through dat_mpesa <br>\n",
    " <br>   \n",
    "4. Summary of Recommendations <br>\n",
    "    4.1 Recommendation - User Acquisition <br>\n",
    "    4.2 Recommendation - Loan Application and Product <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Value Propositions and Assumptions\n",
    "1. If a customer repays his/her first loan, it means he/she either is a good customer or has the need to make loans again in the future. We will analyze the likelihood of repayment based on repayment data of customers' first loans.\n",
    "2. The more frequent the customer uses the loans or the more number of loans the customer starts, the more valuable the customer is. \n",
    "3. A customer that defaults within the first several (no more than 3) loans are considered as a not valuable customer.\n",
    "4. We cannot define whether a customer is valuable if they either just joined the app or only made have very few loans (no more than 3 loans) in his history. \n",
    "5. Customers that made a lot of historical loans or made loans frequently in a period/periods are considered as valuable customers.\n",
    "    1. If a customer churns with default after several loan rounds, he/she is still a valuable customer, but the problem would be that we (the company) fail to offer them customized products. \n",
    "    2. If a customer churned (no matter default or not) after a very frequently loan-making period (but in total maybe only 4 or 5 loans), we still consider this is a valuable customer because he/she demonstrated the need, but it is us that fail to offer good customized loan products to retain him/her. \n",
    "\n",
    "__Note__: These value propositions (assumptions) are the key ideas in this analysis. While some might be subjective and arbitrary, given the limited data volume, these might be the best we can use to generalize insights from the dataset. __This analysis serves as a great proof-of-concept (POC) and can provide guidance for further optimization when having more dataset.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Cleaning & Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Configurate the environment & load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change work directory \n",
    "os.chdir('/Users/craigdavid/Downloads/Facebook Marketing Project/Facebook Marketing Project/Data')\n",
    "# clear warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dat_ppl = pd.read_csv('people.csv')\n",
    "dat_repay = pd.read_csv('repayment.csv')\n",
    "dat_dev = pd.read_csv('device.csv')    \n",
    "dat_mpesa = pd.read_csv('mpesa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Define some functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some frequently used functions.  \n",
    "Please note: most of the functions in this report requires other libraries to be preloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the dataset: count, missing, unique_groups\n",
    "def file_describe(data):\n",
    "    \"\"\"\n",
    "    type: pd.DataFrame\n",
    "    rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    data_describe = pd.DataFrame(data.columns, columns=['colname'])\n",
    "    data_describe['count'] = data_describe['colname'].apply(lambda x: data[x].count())\n",
    "    data_describe['missing'] = data_describe['colname'].apply(lambda x: data[x].isnull().sum())\n",
    "    data_describe['unique_groups'] = data_describe['colname'].apply(lambda x: data[x].unique().size) # include NA\n",
    "    data_describe.set_index('colname', inplace = True)\n",
    "    return(data_describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with median\n",
    "def impute_with_median(data):\n",
    "    \"\"\"\n",
    "    type data: Series \n",
    "    rtype: Series\n",
    "    \"\"\"\n",
    "    data[data.isnull()] = statistics.median(data[data.notnull()])\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data:\n",
    "def standardize(data):\n",
    "    \"\"\"\n",
    "    type data: Series\n",
    "    rtype: Series\n",
    "    \"\"\"\n",
    "    try: \n",
    "        if ((data.isnull().sum()==0) & (np.std(data)>0)):\n",
    "            mu = np.mean(data)\n",
    "            sd = np.std(data) # assume it is not constant\n",
    "            return((data-mu)/sd)\n",
    "        else: \n",
    "            raise ValueError('Input data has NA or is constant')\n",
    "    except:\n",
    "        raise ValueError('Input datatype is not as specified or input data is constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Clean dataset - dat_repay  \n",
    "###### Summarize the process:\n",
    "* Calculate __current loan threshold__:  loan started before (not including) __2015-07-29__ should have been paid back;\n",
    "* Calculate __nRepayed__: number of paid loans through loanPaidDates;\n",
    "* Calculate __nRepayRequired__: number of required loan repayments based on nLoans, start date of last loan and current loan threshold 2015-07-29;\n",
    "* Define different __loan default types__: \n",
    "    * __anyDefault__: if the borrower ultimately defaults;\n",
    "    * __immediateDefault__: if the borrower defaults in the first loan;\n",
    "    * __noFirstLoanDefault__: if the borrower repays in the first loan;\n",
    "    * __within3loansDefault__: if the borrower defaults within the first 3 loans;\n",
    "    * __after7loansDefault__: if the borrower defaults after 7 loans;\n",
    "    * more types ...\n",
    "    \n",
    "__Note__: We will be using __noFirstLoanDefault__ as the target to analyze the likelihood of repayment. As being the target, we will use __immediateDefault__ (the opposite of __noFirstLoanDefault__) to deal with missing values and group very minor buckets in other datasets' mainpulation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>missing</th>\n",
       "      <th>unique_groups</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colname</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>person_id_random</th>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nLoans</th>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loanStartDates</th>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loanPaidDates</th>\n",
       "      <td>727</td>\n",
       "      <td>273</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currentDate</th>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count  missing  unique_groups\n",
       "colname                                        \n",
       "person_id_random   1000        0           1000\n",
       "nLoans             1000        0             24\n",
       "loanStartDates     1000        0            871\n",
       "loanPaidDates       727      273            631\n",
       "currentDate        1000        0              1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, take a look at the data quality \n",
    "dat_repay_describe = file_describe(dat_repay) # using self-defined file_describe function\n",
    "dat_repay_describe\n",
    "# dat_repay.describe(include='all') # pandas built-in describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set person_id_random as the index\n",
    "dat_repay.set_index('person_id_random', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 1: currentDate\n",
    "currentDate = '2015-08-19'\n",
    "# the fintech company allows at most 21 days to pay back\n",
    "currentWindowStart = pd.to_datetime(currentDate) - datetime.timedelta(days = 21) #Timestamp('2015-07-29 00:00:00')\n",
    "# loan started before (not including) currentWindowStart ('2015-07-29') should have been paid back, vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 2: loanPaidDates\n",
    "# var nRepayed: # of loans have been paid\n",
    "dat_repay['nRepayed'] = dat_repay['loanPaidDates'].apply(lambda x: len(x.split(';')) if pd.notnull(x) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 3: loanStartDates\n",
    "# func loan_expire: if a loan should be paid based on the currentDate, return 1, else 0\n",
    "loan_expire = (lambda x: 1 if pd.to_datetime(x) < currentWindowStart else 0) \n",
    "# var nRepayRequired: # of required payment at the currentDate\n",
    "dat_repay['nRepayRequired'] = dat_repay['loanStartDates'].apply(lambda x: len(x.split(';'))\n",
    "                                                                +loan_expire(x.split(';')[-1])-1)\n",
    "# var anyDefault: if nRepayed < nRepayRequired, return True, else False\n",
    "# Note: sometimes we didn't record the repayments correctly, for example, # of loanStartDates >= # of loanPaidDates + 2\n",
    "# Therefore, the condition is modified: (nRepayed < nRepayRequired) & (last loanStartDate < currentWindowStart)\n",
    "# However, id with anyDefault = False does not mean he/she \n",
    "# will not commit default in his/her current loan round (if there is a current round), we has to acccept the error here.\n",
    "dat_repay['anyDefault'] = dat_repay.apply(lambda row: ((row['nRepayRequired']>row['nRepayed']) \n",
    "                                                       & (loan_expire(row['loanStartDates'].split(';')[-1]))), axis = 1)\n",
    "# take a look at the anyDefault:\n",
    "dat_repay.groupby(['anyDefault'])['anyDefault'].count() # default: 560, paid: 440\n",
    "# var immediateDefault: default in the first loan, \n",
    "dat_repay['immediateDefault'] = dat_repay.apply(lambda row: (row['anyDefault'] \n",
    "                                                             & (row['nLoans']==1)), axis=1) #272/560 = 48.6%\n",
    "# var noFirstLoanDefault: no default in the first loan,\n",
    "dat_repay['noFirstLoanDefault'] = dat_repay['immediateDefault'].apply(lambda x: 1-x)\n",
    "# take a look at the anyDefault:\n",
    "dat_repay.groupby(['immediateDefault'])['immediateDefault'].count() # default: 272, paid: 728, base line: 0.272\n",
    "# var within3loansDefault: default within the first 3 loans\n",
    "dat_repay['within3loansDefault'] = dat_repay.apply(lambda row: (row['anyDefault'] \n",
    "                                                                & (row['nLoans']<=3)), axis=1) #439/560 = 78.4%\n",
    "# var after7loansDefault: default after 7 loans,  #36/560 = 6.4%\n",
    "dat_repay['after7loansDefault'] = dat_repay.apply(lambda row: (row['anyDefault'] \n",
    "                                                               & (row['nLoans']>=7)), axis=1) #36/560 = 6.4% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Clean dataset - dat_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summarize the process:\n",
    "* Impute missing values:\n",
    "    * With median of notnull values, given distribution;\n",
    "    * Into a dominant groups; \n",
    "    * Into higher risk groups/classes; \n",
    "    * By creating a new group/class;\n",
    "* Group minor buckets into one:\n",
    "    * Similar/close meaning in practice;\n",
    "    * If they have the similar ratio of the immediateDefault/noFirstLoanDefault (not very minor in this scenario);\n",
    "    * A few opened-ended answers;\n",
    "\n",
    "__Note__: Though technically this is not rigorious if we first impute all the data first and then try to build out a statistical model. In a more rigorous setting, we would divide the dataset into training and test set first. However, given the dataset is small, and what we try to learn is what variables are driving a better performance, it is not a must to have a test set to answer these questions. Instead, we use all the records in the model fitting but we make sure the model does not overfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>missing</th>\n",
       "      <th>unique_groups</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colname</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>person_id_random</th>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birthday</th>\n",
       "      <td>984</td>\n",
       "      <td>16</td>\n",
       "      <td>940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>982</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>signup_date</th>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fb_friend_count</th>\n",
       "      <td>983</td>\n",
       "      <td>17</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>current_home_years</th>\n",
       "      <td>991</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>current_home_months</th>\n",
       "      <td>988</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>living_situation</th>\n",
       "      <td>991</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>current_job_years</th>\n",
       "      <td>767</td>\n",
       "      <td>233</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>current_job_months</th>\n",
       "      <td>755</td>\n",
       "      <td>245</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>740</td>\n",
       "      <td>260</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how_much_paid</th>\n",
       "      <td>987</td>\n",
       "      <td>13</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how_often_paid</th>\n",
       "      <td>986</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>referral_source</th>\n",
       "      <td>807</td>\n",
       "      <td>193</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outstanding_loan</th>\n",
       "      <td>995</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mpesa_how_often</th>\n",
       "      <td>997</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relationship_status</th>\n",
       "      <td>689</td>\n",
       "      <td>311</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_employed</th>\n",
       "      <td>687</td>\n",
       "      <td>313</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count  missing  unique_groups\n",
       "colname                                           \n",
       "person_id_random      1000        0           1000\n",
       "birthday               984       16            940\n",
       "gender                 982       18              3\n",
       "signup_date           1000        0           1000\n",
       "fb_friend_count        983       17            710\n",
       "current_home_years     991        9              7\n",
       "current_home_months    988       12             13\n",
       "living_situation       991        9             21\n",
       "current_job_years      767      233              7\n",
       "current_job_months     755      245             13\n",
       "education              740      260              6\n",
       "how_much_paid          987       13            163\n",
       "how_often_paid         986       14              6\n",
       "referral_source        807      193             17\n",
       "outstanding_loan       995        5              3\n",
       "mpesa_how_often        997        3              6\n",
       "relationship_status    689      311              5\n",
       "is_employed            687      313              3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the data quality\n",
    "dat_ppl_describe = file_describe(dat_ppl)\n",
    "dat_ppl_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set person_id_random as index\n",
    "dat_ppl.set_index('person_id_random', inplace = True)\n",
    "# merge with dat_repay to facilitate the imputation of NA and grouping buckets\n",
    "dat_ppl_repay = pd.merge(dat_ppl, dat_repay, left_index=True, right_index=True, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17381974248927037"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether high/low missing ratio in dat_ppl is highly related with the target: immediateDefault\n",
    "\n",
    "# take a look at the records with high missing ratio in dat_ppl\n",
    "# var numMissing: # of missing fields in the data, attitude-driven records. \n",
    "dat_ppl['numMissing'] = dat_ppl.isnull().sum(axis=1)\n",
    "dat_high_missing_idx = dat_ppl.index[dat_ppl['numMissing']>=8]\n",
    "dat_ppl_repay.loc[dat_high_missing_idx,'immediateDefault'].mean() # 0.22 in 9 record, note the base line is 0.272\n",
    "\n",
    "# take a look at the records with no missing values in dat_ppl\n",
    "dat_no_missing_idx = dat_ppl.index[dat_ppl['numMissing']==0] \n",
    "dat_ppl_repay.loc[dat_no_missing_idx,'immediateDefault'].mean() # 0.174 in 466 record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite counterintuitive. We expect the immediateDefault to be a higher number in high missing ratio records. Though the number is such records is very small and might not be significant when we collect more data, we still could have an idea of what is happening here: sometimes people who have the financially ability and willingness to repay don't care much about the survey questions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 1: birthday\n",
    "# get approximate age base on current date: 2015-08-19\n",
    "dat_ppl['age'] = dat_ppl['birthday'].apply(lambda x: 2015 - int(x.split('/')[0]) if pd.notnull(x) else np.nan)\n",
    "# plt.hist(dat_ppl['age'][dat_ppl['age'].notnull()]), according to figure, impute missing with median\n",
    "dat_ppl['age'] = impute_with_median(dat_ppl['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 2: gender\n",
    "# impute with the majority group\n",
    "dat_ppl.loc[dat_ppl['gender'].isnull(), 'gender'] = 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 3: fb_friend_count\n",
    "# impute with the median, according to figure\n",
    "# plt.hist(dat_ppl['fb_friend_count'][dat_ppl['fb_friend_count'].notnull()]) \n",
    "dat_ppl['fb_friend_count'] = impute_with_median(dat_ppl['fb_friend_count']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 4: education, 26% missing, 6 groups\n",
    "# impute with a new group: 'unknown'\n",
    "dat_ppl.loc[dat_ppl['education'].isnull(), 'education'] = 'unknown'\n",
    "# dat_ppl_repay = pd.merge(dat_ppl, dat_repay, left_index=True, right_index=True, how = 'left')\n",
    "# dat_ppl_repay.groupby(['education'])['immediateDefault'].agg(['mean', 'count'])\n",
    "edu_mapping_list = {'masters': 'college or above', # 0.267, 15\n",
    "                    'college': 'college or above', # 0.177, 515\n",
    "                    'high_school': 'high_school',  # 0.203, 187\n",
    "                    'primary': 'low', # 0.091, 22\n",
    "                    'none': 'none/vacant', # 0, 1\n",
    "                    'unknown': 'none/vacant' # 0.527, 260\n",
    "                    } # potentially could be binary: e.g. if provides and has education information\n",
    "dat_ppl['education'] = dat_ppl['education'].map(edu_mapping_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 5: current_job_years, current_job_months\n",
    "dat_ppl.loc[dat_ppl['current_job_years'].isnull(),'current_job_years'] = 'unknown'\n",
    "dat_ppl.loc[dat_ppl['current_job_years']=='5+','current_job_years'] = '5'\n",
    "dat_ppl.loc[dat_ppl['current_job_months'].isnull(),'current_job_months'] = 'unknown'\n",
    "# var current_job_all_blank: if both current_job_years and current_job_months are missing # 231/1000\n",
    "dat_ppl['current_job_all_blank'] = dat_ppl.apply(lambda row: all(np.in1d(row[['current_job_years', \n",
    "                                                                              'current_job_months']], \n",
    "                                                                         ['unknown'])), axis = 1)\n",
    "# if current_job_years is known, current_job_months is unknown, we impute current_job_months with 0\n",
    "dat_ppl['current_job_months'] = dat_ppl.apply(lambda row: 0 \n",
    "                                              if (row['current_job_years'] != 'unknown')\n",
    "                                              &(row['current_job_months']=='unknown') \n",
    "                                              else row['current_job_months'], \n",
    "                                              axis = 1)\n",
    "# if current_job_months is known, current_job_years is unknown, we impute current_job_years with 0\n",
    "dat_ppl['current_job_years'] = dat_ppl.apply(lambda row: 0 \n",
    "                                             if (row['current_job_months'] != 'unknown')\n",
    "                                             &(row['current_job_years']=='unknown') \n",
    "                                             else row['current_job_years'], \n",
    "                                             axis = 1)\n",
    "# var current_job_years_decimal: aggregate current_job_years and current_job_months, unit: year\n",
    "dat_ppl['current_job_years_decimal'] = dat_ppl.apply(lambda row: int(row['current_job_years']) \n",
    "                                                     + row['current_job_months']/12 \n",
    "                                                     if not row['current_job_all_blank'] \n",
    "                                                     else 'unknown', \n",
    "                                                     axis = 1)\n",
    "# dat_ppl_repay = pd.merge(dat_ppl, dat_repay, left_index=True, right_index=True, how = 'left')\n",
    "# dat_ppl_repay.loc[dat_ppl_repay['current_job_years_decimal']=='unknown', 'immediateDefault'].mean() \n",
    "# 0.186 less than base line: 0.56\n",
    "# actually ppl without providing current_job_years/months have smaller default rate\n",
    "# we filling the missing with median, but as 'unknown' has # of 226, this field should not be useful  \n",
    "dat_ppl['current_job_years_decimal'] = impute_with_median(dat_ppl['current_job_years_decimal'].apply(lambda x: np.nan \n",
    "                                                                                                     if x=='unknown' \n",
    "                                                                                                     else float(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 6: current_home_years, current_home_months\n",
    "dat_ppl.loc[dat_ppl['current_home_years'].isnull(),'current_home_years'] = 'unknown'\n",
    "dat_ppl.loc[dat_ppl['current_home_years']=='5+','current_home_years'] = '5'\n",
    "dat_ppl.loc[dat_ppl['current_home_months'].isnull(),'current_home_months'] = 'unknown'\n",
    "# var current_home_all_blank: if both current_home_years and current_home_months are missing # 9\n",
    "dat_ppl['current_home_all_blank'] = dat_ppl.apply(lambda row: all(np.in1d(row[['current_home_years', \n",
    "                                                                               'current_home_months']], \n",
    "                                                                          ['unknown'])), axis = 1)\n",
    "# if current_home_years is known, current_home_months is unknown, we impute current_home_months with 0\n",
    "dat_ppl['current_home_months'] = dat_ppl.apply(lambda row: 0 \n",
    "                                               if (row['current_home_years'] != 'unknown')\n",
    "                                               &(row['current_home_months']=='unknown') \n",
    "                                               else row['current_home_months'], \n",
    "                                               axis = 1)\n",
    "# if current_home_months is known, current_home_years is unknown, we impute current_home_years with 0\n",
    "dat_ppl['current_home_years'] = dat_ppl.apply(lambda row: 0 \n",
    "                                              if (row['current_home_months'] != 'unknown')\n",
    "                                              &(row['current_home_years']=='unknown') \n",
    "                                              else row['current_home_years'], \n",
    "                                              axis = 1)\n",
    "# var current_home_years_decimal: aggregate current_home_years and current_home_months, unit: year\n",
    "dat_ppl['current_home_years_decimal'] = dat_ppl.apply(lambda row: int(row['current_home_years']) \n",
    "                                                      + row['current_home_months']/12 \n",
    "                                                      if not row['current_home_all_blank'] \n",
    "                                                      else 'unknown', \n",
    "                                                      axis = 1)\n",
    "# as dat_ppl['current_home_all_blank'].sum() = 9, we filling the missing with median\n",
    "dat_ppl['current_home_years_decimal'] = impute_with_median(dat_ppl['current_home_years_decimal'].apply(lambda x: np.nan \n",
    "                                                                                                       if x=='unknown' \n",
    "                                                                                                       else x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 7: referral_source\n",
    "# npte: if no referral_source (193), the chance of default is extremely high\n",
    "# dat_ppl_repay.immediateDefault[dat_ppl_repay['referral_source'].isnull()].mean() # 0.705\n",
    "dat_ppl_repay.groupby(['referral_source'])['immediateDefault'].agg(['mean', 'count'])\n",
    "# var if_referred: if referred, return 1, vice versa\n",
    "dat_ppl['if_referred'] = dat_ppl['referral_source'].apply(lambda x: pd.notnull(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 8: is_employed\n",
    "dat_ppl['is_employed'].isnull().sum() # 313\n",
    "# missing values correspond with significant high default rate 0.476\n",
    "dat_ppl_repay.immediateDefault[dat_ppl_repay['is_employed'].isnull()].mean() \n",
    "# yes 0.180, no 0.175 # and substantially below base line 0.272\n",
    "dat_ppl_repay.groupby(['is_employed'])['immediateDefault'].agg(['mean', 'count']) \n",
    "# impute na with 'unknown'\n",
    "dat_ppl.loc[dat_ppl['is_employed'].isnull(), 'is_employed'] = 'unknown'\n",
    "# dat_ppl_repay.loc[dat_ppl_repay['is_employed'].isnull(), 'is_employed'] = 'unknown'\n",
    "# dat_ppl_repay.groupby(['is_employed'])['immediateDefault'].agg(['mean', 'count']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 9: relationship_status\n",
    "dat_ppl['relationship_status'].isnull().sum() # 311\n",
    "# missing values correspond with significant high default rate 0.479\n",
    "dat_ppl_repay.immediateDefault[dat_ppl_repay['relationship_status'].isnull()].mean() \n",
    "# no much difference, and quite low\n",
    "dat_ppl_repay.groupby(['relationship_status'])['immediateDefault'].agg(['mean', 'count']) \n",
    "# impute na with 'unknown'\n",
    "dat_ppl.loc[dat_ppl['relationship_status'].isnull(), 'relationship_status'] = 'unknown'\n",
    "# group married and alternative together as married_alternative: financially good\n",
    "dat_ppl.loc[dat_ppl['relationship_status'].isin(['alternative', 'married']), 'relationship_status'] = 'married_alternative'\n",
    "# group single and long-term relationship as single_longterm: financially not good enough for date/marriage\n",
    "dat_ppl.loc[dat_ppl['relationship_status'].isin(['single', 'long-term']), 'relationship_status'] = 'single_longterm'\n",
    "# dat_ppl_repay = pd.merge(dat_ppl, dat_repay, left_index=True, right_index=True, how = 'left')\n",
    "# dat_ppl_repay.groupby(['relationship_status'])['immediateDefault'].agg(['mean', 'count']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 10: living_situation\n",
    "dat_ppl['living_situation'].isnull().sum() # 9\n",
    "dat_ppl_repay.immediateDefault[dat_ppl_repay['living_situation'].isnull()].mean() # 0.22 - only 9 samples\n",
    "dat_ppl_repay.groupby(['living_situation'])['immediateDefault'].agg(['mean', 'count']) # no much difference & quite low\n",
    "# func living_sit_map: map the living_situation to 4 groups\n",
    "def living_sit_map(x):\n",
    "    if pd.isnull(x):\n",
    "        return('other') # impute missing value into the highest default rate group\n",
    "    elif x in ['pay_rent']:\n",
    "        return('pay_rent')\n",
    "    elif x in ['own_home']:\n",
    "        return('own_home')\n",
    "    elif x in ['with_family', 'student_housing', 'provided for', \n",
    "               'Live with my parent', 'I live in my parental home']:\n",
    "        return('family&stdhouse')\n",
    "    else:\n",
    "        return('other')\n",
    "dat_ppl['living_situation'] = dat_ppl['living_situation'].apply(lambda x: living_sit_map(x))\n",
    "# dat_ppl_repay = pd.merge(dat_ppl, dat_repay, left_index=True, right_index=True, how = 'left')\n",
    "# dat_ppl_repay.groupby(['living_situation'])['immediateDefault'].agg(['mean', 'count']) # no much difference, and quite low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 11: mpesa_how_often\n",
    "dat_ppl['mpesa_how_often'].isnull().sum() # 9\n",
    "# impute na with the majority group\n",
    "dat_ppl.loc[dat_ppl['mpesa_how_often'].isnull(),'mpesa_how_often'] = 'few_times_a_week' \n",
    "# combine the high-frequent group into one: '10+', '4-9', 'daily', \n",
    "dat_ppl.loc[dat_ppl['mpesa_how_often'].isin(['10+', '4-9']), 'mpesa_how_often'] = 'daily'\n",
    "# dat_ppl_repay = pd.merge(dat_ppl, dat_repay, left_index=True, right_index=True, how = 'left')\n",
    "# dat_ppl_repay.groupby(['mpesa_how_often'])['immediateDefault'].agg(['mean', 'count']) \n",
    "# almost no difference between groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 12: outstanding_loan\n",
    "dat_ppl['outstanding_loan'].isnull().sum() # 5\n",
    "dat_ppl_repay.immediateDefault[dat_ppl_repay['outstanding_loan'].isnull()].mean() # 0.4 in 5, not a big concern given sample size\n",
    "dat_ppl.outstanding_loan.describe()\n",
    "dat_ppl_repay.groupby(['outstanding_loan'])['immediateDefault'].agg(['mean', 'count']) # no: 0.31, yes = 0.19\n",
    "# impute na with higher risk group\n",
    "dat_ppl.loc[dat_ppl['outstanding_loan'].isnull(),'outstanding_loan'] = 'no' # assign to group with higher risk \n",
    "# dat_ppl_repay = pd.merge(dat_ppl, dat_repay, left_index=True, right_index=True, how = 'left')\n",
    "# dat_ppl_repay.groupby(['outstanding_loan'])['immediateDefault'].agg(['mean', 'count']) # no: 0.31, yes = 0.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 13: how_much_paid\n",
    "dat_ppl['how_much_paid'].isnull().sum() # 13\n",
    "dat_ppl_repay.immediateDefault[dat_ppl_repay['how_much_paid'].isnull()].mean() # 0.23 in 13\n",
    "dat_ppl.how_much_paid.describe()\n",
    "# impute na with median\n",
    "dat_ppl['how_much_paid'] = impute_with_median(dat_ppl['how_much_paid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 14: how_often_paid\n",
    "dat_ppl['how_often_paid'].isnull().sum() # 14\n",
    "dat_ppl_repay.immediateDefault[dat_ppl_repay['how_often_paid'].isnull()].mean() # 0.285 in 14, not a big concern\n",
    "dat_ppl_repay.groupby(['how_often_paid'])['immediateDefault'].agg(['mean', 'count'])\n",
    "# impute na with the majority group\n",
    "dat_ppl.loc[dat_ppl['how_often_paid'].isnull(),'how_often_paid'] = 'monthly'  \n",
    "# combine the minority group into 'daily' (higher risk)\n",
    "dat_ppl.loc[~dat_ppl['how_often_paid'].isin(['daily', 'weekly', 'monthly']),'how_often_paid'] = 'daily' \n",
    "# dat_ppl_repay = pd.merge(dat_ppl, dat_repay, left_index=True, right_index=True, how = 'left')\n",
    "# dat_ppl_repay.groupby(['how_often_paid'])['immediateDefault'].agg(['mean', 'count']) \n",
    "# monthly has slightly small immediateDefault rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Clean dataset - dat_dev\n",
    "###### Summarize the process:\n",
    "* Impute missing values or delete missing records based on rules:\n",
    "* Group minor buckets into one:\n",
    "* Leave device_model aside at this stage due to its complexity\n",
    "    * it will be very helpful to just have the price of each device_model since this is a good indicator of the borrower's financial wellness. \n",
    "* Set up loan application rules: \n",
    "    * __reject loan applications if we don't have enough or any device information__, as missing/no device information is highly correlated with 1st-loan-default. By simplicity, we should have some simple (but robust) rules to reduce the risk substantially.\n",
    "    * these rules could be improved a lot given more dataset.\n",
    "\n",
    "__Note__: However, in the survey data, aka, dat_ppl, we cannot do this if people provide less info, because some people, who has the will and ability to pay for the money, might not take the survey for serious. It's true that they need the money, but they are not begging for it to conduct default. And if this is true, they are potentially very valuable customers who frequently take loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>missing</th>\n",
       "      <th>unique_groups</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colname</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>person_id_random</th>\n",
       "      <td>973</td>\n",
       "      <td>0</td>\n",
       "      <td>973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>device_model</th>\n",
       "      <td>972</td>\n",
       "      <td>1</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasDualSim</th>\n",
       "      <td>972</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasWlan</th>\n",
       "      <td>973</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasBluetooth</th>\n",
       "      <td>973</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mkopo_version_name</th>\n",
       "      <td>972</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>android_version</th>\n",
       "      <td>972</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count  missing  unique_groups\n",
       "colname                                          \n",
       "person_id_random      973        0            973\n",
       "device_model          972        1            254\n",
       "hasDualSim            972        1              3\n",
       "hasWlan               973        0              2\n",
       "hasBluetooth          973        0              2\n",
       "mkopo_version_name    972        1             12\n",
       "android_version       972        1             22"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a brief look at the data quality\n",
    "dat_dev_describe = file_describe(dat_dev)\n",
    "dat_dev_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the person_id_random as index\n",
    "dat_dev.set_index('person_id_random', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since 27 records are missing in the device dataset\n",
    "# We assume these are caused by failing to get the device data from the users' sides\n",
    "# So intuitively these users are more suspicious in behavior, not sharing the device information;\n",
    "\n",
    "# analyze missing person_id_random, 27 records\n",
    "no_device_id = set(dat_repay.index) - set(dat_dev.index)\n",
    "no_device_hist = dat_repay.loc[no_device_id,:]\n",
    "no_device_hist['immediateDefault'].mean() # 0.778, if no device info, high default rate\n",
    "# we should have a rule: if no enough device info, reject the loan application (*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Insights__: Since missing device information will drive a 1st-loan-default ratio of 78%, we should set a rule: __*reject loan applications if we don't have enough or any device information * __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 1: device_model\n",
    "dat_dev.loc[dat_dev['device_model'].isnull(),:] \n",
    "# drop this record as well, considering the rule of enough device information required\n",
    "drop_index = list(dat_dev.index[dat_dev['device_model'].isnull()])\n",
    "dat_dev.drop(drop_index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# var 2: hasDualSim\n",
    "dat_dev['hasDualSim'].isnull().sum() # 0\n",
    "# dat_dev.loc[dat_dev['device_model'].isnull(),'hasDualSim'] = 0 # missing rule for future records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 3: mkopo_version_name\n",
    "dat_dev['mkopo_version_name'] = dat_dev['mkopo_version_name'].astype(str)\n",
    "# dat_dev_repay = pd.merge(dat_dev, dat_repay, left_index=True, right_index=True, how = 'left')\n",
    "# dat_dev_repay.groupby(['mkopo_version_name'])['immediateDefault'].agg(['mean', 'count']) # more latest version -> less default rate\n",
    "# func mkopo_map: map mkopo_version_name to 3 groups \n",
    "def mkopo_map(version):\n",
    "    if version<='3.0':\n",
    "        return('old')\n",
    "    elif version<='3.3':\n",
    "        return('medium')\n",
    "    else:\n",
    "        return('advanced')\n",
    "# map the mkopo_version_name\n",
    "dat_dev['mkopoVer'] = dat_dev['mkopo_version_name'].apply(lambda x: mkopo_map(x))\n",
    "# dat_dev_repay = pd.merge(dat_dev, dat_repay, left_index=True, right_index=True, how = 'left')\n",
    "# dat_dev_repay.groupby(['mkopoVer'])['immediateDefault'].agg(['mean', 'count']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 4: android_version\n",
    "# dat_dev_repay = pd.merge(dat_dev, dat_repay, left_index=True, right_index=True, how = 'left')\n",
    "# dat_dev_repay.groupby(['android_version'])['immediateDefault'].agg(['mean', 'count']) # more advanced version -> less default rate\n",
    "# func android_map: map android_version to 3 groups\n",
    "def android_map(version):\n",
    "    if version[0:3]<='4.0':\n",
    "        return('old')\n",
    "    elif version[0:3]<='4.4':\n",
    "        return('medium')\n",
    "    else:\n",
    "        return('advanced')\n",
    "# map the android version\n",
    "dat_dev['andrVer'] = dat_dev['android_version'].apply(lambda x: android_map(x))\n",
    "# dat_dev_repay = pd.merge(dat_dev, dat_repay, left_index=True, right_index=True, how = 'left')\n",
    "# dat_dev_repay.groupby(['andrVer'])['immediateDefault'].agg(['mean', 'count']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Clean dataset - dat_mpesa\n",
    "###### Summarize the process:\n",
    "* Transform the information from transaction level to customer level;\n",
    "    * person_id_random\n",
    "    * count: total transaction number (at most 30)\n",
    "    * num_out: total transaction out number \n",
    "    * num_in: total transaction in number \n",
    "    * net_count: (num_out-num_in)/count \n",
    "    * avg_amt_out: avg out amount\n",
    "    * avg_amt_in: avg in amount\n",
    "    * net_amt_bin: if avg_amt_out>avg_amt_in\n",
    "\n",
    "__Note__: This dataset helps understand and profile the customers, but considering there is a clear rule in sampling the dataset: the maximum number of transaction provided here is limited to 30. So we should be conservative and only work on simple analyses here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 4: dat_mpesa (hold)\n",
    "# some people registered but haven't started any transactions/loads\n",
    "dat_mpesa_describe = file_describe(dat_mpesa)\n",
    "dat_mpesa_describe\n",
    "# impute with median as the missing is minor\n",
    "dat_mpesa['amount'] = impute_with_median(dat_mpesa['amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall\n",
    "dat_mpesa_all = dat_mpesa.groupby(['person_id_random'])['person_id_random'].agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset - out\n",
    "dat_mpesa_out = dat_mpesa.loc[dat_mpesa.direction == 'out', :]\n",
    "dat_mpesa_out = dat_mpesa_out.groupby(['person_id_random'])['amount'].agg(['count', 'mean'])\n",
    "dat_mpesa_out.columns = ['num_out', 'avg_amt_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset - in\n",
    "dat_mpesa_in = dat_mpesa.loc[dat_mpesa.direction == 'in', :]\n",
    "dat_mpesa_in = dat_mpesa_in.groupby(['person_id_random'])['amount'].agg(['count', 'mean'])\n",
    "dat_mpesa_in.columns = ['num_in', 'avg_amt_in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge together into a person_id level\n",
    "dat_mpesa_id = pd.merge(dat_mpesa_all, dat_mpesa_out, left_index = True, right_index = True, how = 'left')\n",
    "dat_mpesa_id = pd.merge(dat_mpesa_id, dat_mpesa_in, left_index = True, right_index = True, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter records with na\n",
    "# since there is a limit of data collect, assuming NAs are caused by random\n",
    "dat_mpesa_id.drop(dat_mpesa_id.index[dat_mpesa_id.isnull().sum(axis=1)>=1], inplace = True) # 845 id left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net columns\n",
    "dat_mpesa_id['net_count'] = (dat_mpesa_id['num_out'] > dat_mpesa_id['num_in'])/dat_mpesa_id['count']\n",
    "dat_mpesa_id['net_amt_bin'] = dat_mpesa_id['avg_amt_out'] > dat_mpesa_id['avg_amt_in']\n",
    "# all net_count >=0, 395/845 are equal to 0\n",
    "# 347 net_amt_bin = True, 498 net_amt_bin is False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Modeling the Likelihood of Repayment\n",
    "\n",
    "As we discuss above, the __noFirstLoanDefault__ will be the key indicator in analyzing the __likelihood of repayment__. Recall the __value proposition__ that __if a customer repays his/her 1st loan, it means he/she either is a good customer or has the need to make loans in the future__. On the contrary, if a customer does not repay the 1st loan, it is very likely that they will not repay if offered another loan.   \n",
    "\n",
    "Honestly, __some people that repay the first loan would default in future loans__. According to this dataset, 560 people default eventually while only 272 of them default in the first loans. If the assumption __\"for each borrower, his/her loans are independent from one another\"__ is correct, then __noFirstLoanDefault__ would be a almost perfect indicator to measure the likelihood of repayment. But the truth is that this assumption is very strong, especially for the lending business. \n",
    "\n",
    "We could construct a complexy metrics based on a combination of many rules to analyze the likelihood of repayment. But this __noFirstLoanDefault__ is a simple and clean way to measure the repayment and help with interpretation in nature. So we move forward with it.  \n",
    "\n",
    "* __Approach: analyze statistics in logistic regression result__  \n",
    "* __Target: fit a model that predict the noFirstLoanDefault__\n",
    "\n",
    "\n",
    "\n",
    "__Note__: We do construct a complexity metrics called __life-time-value (LTV)__ in the Part 3 based on a combination of many rules. We once again put it as a __binary variable__ just as __noFirstLoanDefault__ because the data volume is small and the __discrete metrics would be very volatile__. But that is not directly related with __likelihood of repayment__. Because in the part 3, we want to maximize the repayment outcomes and continuing using the __noFirstLoanDefault__ does not tell anything about the lifetime repayment history. We wil be talking about details in the __Part 3__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Prepare the combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dat_ppl, dat_repay, dat_dev\n",
    "dat = pd.merge(dat_ppl, dat_dev, left_index=True, right_index=True, how = 'left')\n",
    "dat = pd.merge(dat, dat_repay, left_index=True, right_index=True, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(972, 43)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter some records - based on rules\n",
    "# rule 1: filter records if no device info. 27+1 in this case\n",
    "dat = dat.loc[dat.isnull().sum(axis=1)<5,:].copy()\n",
    "# rule 2: filter records if numMissing is too high\n",
    "# we already impute this field, we hold this filtering later!\n",
    "# dat1 = dat.loc[dat.numMissing<=8,:].copy() # 963 # Optional filteirng\n",
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the merged clean file just in case\n",
    "dat.to_csv('dat_noFirstLoanDefault_' + \n",
    "                 datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\") + \n",
    "                 \".csv\", index = True,\n",
    "                 encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unuseful columns\n",
    "drop_list = ['birthday', \n",
    "             'signup_date', \n",
    "             'referral_source',\n",
    "             'current_home_years', \n",
    "             'current_home_months',\n",
    "             'current_job_years', \n",
    "             'current_job_months',\n",
    "             'current_job_all_blank', \n",
    "             'current_home_all_blank',\n",
    "             'nRepayRequired',\n",
    "             'device_model', \n",
    "             'mkopo_version_name', \n",
    "             'android_version']\n",
    "dat.drop(drop_list, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Modeling the likelihood of repayment\n",
    "\n",
    "Since __the likelihood of noFirstLoanDefault (Repayment) + the likelihood of immediateDefault = 1__, we have the variable called 'noFirstLoanDefault' will be the target in this logistic regression model.  \n",
    "\n",
    "Facebook only offers filters for __(1) mobile device type, (2) minimum Android OS, (3) wireless connection type (mobile, wifi), (4) gender, (5) age, (6) education, (7) work, (8) income, and (9) homeownership__. \n",
    "\n",
    "So we will be doing the logistic regression purely based on these informations.  \n",
    "\n",
    "We choose to neglect the variable mobile device type given it complexity. And we believe that (7) and (9) could be provided as granular as 'is_employed' or 'living_situation', instead of current_job_years/months and current_home_years/months. We move forward with this the following variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the information we include in the predictive model.\n",
    "var = ['gender', # binary ['M', 'F']\n",
    "       'living_situation', # 4 levels ['other', 'pay_rent', 'family&stdhouse', 'own_home']\n",
    "       'education', # 4 levels ['none/vacant', 'college or above', 'high_school', 'low']\n",
    "       'how_much_paid', # numeric\n",
    "       'is_employed', # 3 levels ['unknown', 'yes', 'no']\n",
    "       'age', # numeric\n",
    "       'hasWlan', # binary \n",
    "       'andrVer' # 3 levels ['old', 'medium', 'advanced']\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy the var if applicable \n",
    "dat_model = dat[var].copy()\n",
    "dat_model = pd.get_dummies(dat_model, drop_first=False)\n",
    "# set the base of each categorical variables by dropping them \n",
    "drop_var_list = ['gender_M', \n",
    "                 'living_situation_pay_rent',\n",
    "                 'education_college or above',\n",
    "                 'andrVer_old',\n",
    "                 'is_employed_unknown'\n",
    "                 ]\n",
    "dat_model.drop(drop_var_list, axis = 1, inplace = True)\n",
    "dat_model.shape # (972, 14)\n",
    "# standardize how_much_paid, \n",
    "dat_model['how_much_paid'] = standardize(dat_model['how_much_paid'].apply(lambda x: math.log(x+1)))\n",
    "# plt.hist(dat_model['how_much_paid'], bins =100) # normal distributed\n",
    "# standardize age\n",
    "dat_model['age'] = standardize(dat_model['age'])\n",
    "# append the target column: noFirstLoanDefault\n",
    "dat_model['Repayment'] = dat['noFirstLoanDefault']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation\n",
    "corr_matrix = dat_model.corr() # correlation checked, decent enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.512592\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>       <td>No. Iterations:</td>   <td>7.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>Repayment</td>    <td>Pseudo R-squared:</td>   <td>0.101</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2018-07-16 23:59</td>       <td>AIC:</td>        <td>1026.4795</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>972</td>             <td>BIC:</td>        <td>1099.6698</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>14</td>         <td>Log-Likelihood:</td>   <td>-498.24</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>957</td>           <td>LL-Null:</td>       <td>-554.15</td> \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>        <td>1.0000</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                  <td></td>                  <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>how_much_paid</th>                    <td>0.1902</td>   <td>0.0827</td>  <td>2.3011</td>  <td>0.0214</td> <td>0.0282</td>  <td>0.3523</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>                              <td>0.0503</td>   <td>0.0709</td>  <td>0.7090</td>  <td>0.4783</td> <td>-0.0887</td> <td>0.1893</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hasWlan</th>                          <td>0.3658</td>   <td>0.1769</td>  <td>2.0677</td>  <td>0.0387</td> <td>0.0191</td>  <td>0.7125</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender_F</th>                         <td>0.3078</td>   <td>0.1826</td>  <td>1.6853</td>  <td>0.0919</td> <td>-0.0502</td> <td>0.6657</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>living_situation_family&stdhouse</th> <td>0.1082</td>   <td>0.3274</td>  <td>0.3303</td>  <td>0.7412</td> <td>-0.5336</td> <td>0.7499</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>living_situation_other</th>           <td>0.6901</td>   <td>0.4486</td>  <td>1.5385</td>  <td>0.1239</td> <td>-0.1891</td> <td>1.5693</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>living_situation_own_home</th>        <td>0.2811</td>   <td>0.2218</td>  <td>1.2674</td>  <td>0.2050</td> <td>-0.1536</td> <td>0.7158</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_high_school</th>            <td>-0.0476</td>  <td>0.2215</td>  <td>-0.2151</td> <td>0.8297</td> <td>-0.4818</td> <td>0.3865</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_low</th>                    <td>0.8671</td>   <td>0.7554</td>  <td>1.1479</td>  <td>0.2510</td> <td>-0.6134</td> <td>2.3476</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_none/vacant</th>            <td>-1.0048</td>  <td>0.3673</td>  <td>-2.7355</td> <td>0.0062</td> <td>-1.7247</td> <td>-0.2849</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>is_employed_no</th>                   <td>0.4627</td>   <td>0.3794</td>  <td>1.2196</td>  <td>0.2226</td> <td>-0.2809</td> <td>1.2062</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>is_employed_yes</th>                  <td>0.2640</td>   <td>0.3524</td>  <td>0.7493</td>  <td>0.4537</td> <td>-0.4266</td> <td>0.9547</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>andrVer_advanced</th>                 <td>1.8973</td>   <td>1.0533</td>  <td>1.8013</td>  <td>0.0717</td> <td>-0.1671</td> <td>3.9617</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>andrVer_medium</th>                   <td>0.3857</td>   <td>0.2055</td>  <td>1.8767</td>  <td>0.0606</td> <td>-0.0171</td> <td>0.7885</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>                        <td>0.4002</td>   <td>0.3990</td>  <td>1.0030</td>  <td>0.3159</td> <td>-0.3818</td> <td>1.1822</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                                 Results: Logit\n",
       "================================================================================\n",
       "Model:                    Logit                 No. Iterations:        7.0000   \n",
       "Dependent Variable:       Repayment             Pseudo R-squared:      0.101    \n",
       "Date:                     2018-07-16 23:59      AIC:                   1026.4795\n",
       "No. Observations:         972                   BIC:                   1099.6698\n",
       "Df Model:                 14                    Log-Likelihood:        -498.24  \n",
       "Df Residuals:             957                   LL-Null:               -554.15  \n",
       "Converged:                1.0000                Scale:                 1.0000   \n",
       "--------------------------------------------------------------------------------\n",
       "                                  Coef.  Std.Err.    z    P>|z|   [0.025  0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "how_much_paid                     0.1902   0.0827  2.3011 0.0214  0.0282  0.3523\n",
       "age                               0.0503   0.0709  0.7090 0.4783 -0.0887  0.1893\n",
       "hasWlan                           0.3658   0.1769  2.0677 0.0387  0.0191  0.7125\n",
       "gender_F                          0.3078   0.1826  1.6853 0.0919 -0.0502  0.6657\n",
       "living_situation_family&stdhouse  0.1082   0.3274  0.3303 0.7412 -0.5336  0.7499\n",
       "living_situation_other            0.6901   0.4486  1.5385 0.1239 -0.1891  1.5693\n",
       "living_situation_own_home         0.2811   0.2218  1.2674 0.2050 -0.1536  0.7158\n",
       "education_high_school            -0.0476   0.2215 -0.2151 0.8297 -0.4818  0.3865\n",
       "education_low                     0.8671   0.7554  1.1479 0.2510 -0.6134  2.3476\n",
       "education_none/vacant            -1.0048   0.3673 -2.7355 0.0062 -1.7247 -0.2849\n",
       "is_employed_no                    0.4627   0.3794  1.2196 0.2226 -0.2809  1.2062\n",
       "is_employed_yes                   0.2640   0.3524  0.7493 0.4537 -0.4266  0.9547\n",
       "andrVer_advanced                  1.8973   1.0533  1.8013 0.0717 -0.1671  3.9617\n",
       "andrVer_medium                    0.3857   0.2055  1.8767 0.0606 -0.0171  0.7885\n",
       "intercept                         0.4002   0.3990  1.0030 0.3159 -0.3818  1.1822\n",
       "================================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append intercept as 1\n",
    "dat_model['intercept'] = 1   \n",
    "# run a logistic regression\n",
    "model = sm.Logit(dat_model['Repayment'], dat_model.loc[:, dat_model.columns != 'Repayment'])\n",
    "result = model.fit(maxiter=100)\n",
    "result.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that **is_employed_unknown** (1-is_employed_no-is_employed_yes) is highly correlated (corr=0.87) with **education_none/vacant**, so there could be multicollinearity in this problem. we need to iterate to do feature selection and make sure **is_employed_no**, **is_employed_yes** and **education_none/vacant** will not all appear in a same result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.515648\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>       <td>No. Iterations:</td>   <td>7.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>Repayment</td>    <td>Pseudo R-squared:</td>   <td>0.096</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2018-07-16 23:59</td>       <td>AIC:</td>        <td>1018.4205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>972</td>             <td>BIC:</td>        <td>1057.4553</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>7</td>         <td>Log-Likelihood:</td>   <td>-501.21</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>964</td>           <td>LL-Null:</td>       <td>-554.15</td> \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>        <td>1.0000</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>             <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>how_much_paid</th>          <td>0.1771</td>   <td>0.0751</td>  <td>2.3577</td>  <td>0.0184</td> <td>0.0299</td>  <td>0.3243</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hasWlan</th>                <td>0.3680</td>   <td>0.1759</td>  <td>2.0916</td>  <td>0.0365</td> <td>0.0232</td>  <td>0.7128</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender_F</th>               <td>0.3001</td>   <td>0.1802</td>  <td>1.6652</td>  <td>0.0959</td> <td>-0.0531</td> <td>0.6533</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>living_situation_other</th> <td>0.6358</td>   <td>0.4475</td>  <td>1.4207</td>  <td>0.1554</td> <td>-0.2413</td> <td>1.5129</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_none/vacant</th>  <td>-1.3227</td>  <td>0.1833</td>  <td>-7.2154</td> <td>0.0000</td> <td>-1.6820</td> <td>-0.9634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>andrVer_advanced</th>       <td>1.8499</td>   <td>1.0514</td>  <td>1.7595</td>  <td>0.0785</td> <td>-0.2108</td> <td>3.9106</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>andrVer_medium</th>         <td>0.3856</td>   <td>0.2038</td>  <td>1.8920</td>  <td>0.0585</td> <td>-0.0138</td> <td>0.7851</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>              <td>0.7657</td>   <td>0.2362</td>  <td>3.2411</td>  <td>0.0012</td> <td>0.3027</td>  <td>1.2287</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                            Results: Logit\n",
       "======================================================================\n",
       "Model:                Logit              No. Iterations:     7.0000   \n",
       "Dependent Variable:   Repayment          Pseudo R-squared:   0.096    \n",
       "Date:                 2018-07-16 23:59   AIC:                1018.4205\n",
       "No. Observations:     972                BIC:                1057.4553\n",
       "Df Model:             7                  Log-Likelihood:     -501.21  \n",
       "Df Residuals:         964                LL-Null:            -554.15  \n",
       "Converged:            1.0000             Scale:              1.0000   \n",
       "----------------------------------------------------------------------\n",
       "                        Coef.  Std.Err.    z    P>|z|   [0.025  0.975]\n",
       "----------------------------------------------------------------------\n",
       "how_much_paid           0.1771   0.0751  2.3577 0.0184  0.0299  0.3243\n",
       "hasWlan                 0.3680   0.1759  2.0916 0.0365  0.0232  0.7128\n",
       "gender_F                0.3001   0.1802  1.6652 0.0959 -0.0531  0.6533\n",
       "living_situation_other  0.6358   0.4475  1.4207 0.1554 -0.2413  1.5129\n",
       "education_none/vacant  -1.3227   0.1833 -7.2154 0.0000 -1.6820 -0.9634\n",
       "andrVer_advanced        1.8499   1.0514  1.7595 0.0785 -0.2108  3.9106\n",
       "andrVer_medium          0.3856   0.2038  1.8920 0.0585 -0.0138  0.7851\n",
       "intercept               0.7657   0.2362  3.2411 0.0012  0.3027  1.2287\n",
       "======================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional feature selection\n",
    "# we drop variable with the highest p-value each time until every variable is significant\n",
    "# This further reduce the overfitting and improve generalization power\n",
    "# run a logistic regression\n",
    "model = sm.Logit(dat_model['Repayment'], \n",
    "                 dat_model.loc[:, ~(dat_model.columns.isin(['Repayment', \n",
    "                                                            'education_high_school',\n",
    "                                                            'living_situation_family&stdhouse',\n",
    "                                                            'is_employed_yes',\n",
    "                                                            'age',\n",
    "                                                            'is_employed_no',\n",
    "                                                            'education_low',\n",
    "                                                            'living_situation_own_home']))])\n",
    "result = model.fit(maxiter=100)\n",
    "result.summary2() # the result meets expectations\n",
    "# we don't exclude living_situation_other as it explains a potential direction of user segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Interpret this summary:__  \n",
    "    * __how much paid__, has a P-value of 0.0124 and a positive coef. It means the higher the customer gets paid, the more likely he/she will repay.   \n",
    "    * __gender female__, has a P-value of 0.09 and a postive coef. It means the female customers are more likely to repay than male customers. Though the p-value is not ideally small to conclude with more confidence, it still provides a good direction that it is suggested to target female audience rather than male audience.   \n",
    "    * __education none or vacant__, has a p-value of 0.0062 and a large negative coef. It means people don't fill this field in the application are significantly less likely to repay. \n",
    "    * As __is employed unknown__ is highly correlated with __education none or vacant__, this also suggest that people don't fill is_employed in the application are significantly less likely to repay.  \n",
    "    * __andriod version__, both 'advanced' and 'medium' have small p-values and positive coefs. We can see that advanced andriod version has a very large coef and while medium andriod version has a relative smaller one. Since the base is the old andriod version, this simply means the higher andriod version, the more likely of repayment.  \n",
    "    * __hasWlan__, has a positive coef and a very small p-value. It means hasWlan = 1 is more likely to drive higher likelihood of repayment.    \n",
    "    * __age__, and __living_situation__ do not show a statistically significant impact on the repayment likelihood.  \n",
    "    \n",
    "In the real world, __making decisions through blur observations__ is very important. Though not very significant statistically, for exmaple, p-value slightly larger than 0.05, it is still useful to have a sense of how to make the decision if there has to be one.   \n",
    "    \n",
    "As the dataset contains 972 records, which is a relatively high record-variable ratio for logisic regression. Overfitting is not a major concern here. So we don't partition the data into training and test. Also, we care more about the coef of each variable rather than whether the model achieves its best predictive power. Simply fitting all the dataset with logistic regression would be enough to generalize the direction of the variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Support the decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0.771536</td>\n",
       "      <td>206</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.731915</td>\n",
       "      <td>516</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean  sum  count\n",
       "gender                      \n",
       "F       0.771536  206    267\n",
       "M       0.731915  516    705"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.groupby(['gender'])['noFirstLoanDefault'].agg(['mean', 'sum', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>andrVer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>advanced</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>0.776215</td>\n",
       "      <td>607</td>\n",
       "      <td>782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <td>0.562130</td>\n",
       "      <td>95</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mean  sum  count\n",
       "andrVer                       \n",
       "advanced  0.952381   20     21\n",
       "medium    0.776215  607    782\n",
       "old       0.562130   95    169"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.groupby(['andrVer'])['noFirstLoanDefault'].agg(['mean', 'sum', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasWlan</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.675325</td>\n",
       "      <td>156</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.763833</td>\n",
       "      <td>566</td>\n",
       "      <td>741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mean  sum  count\n",
       "hasWlan                      \n",
       "0.0      0.675325  156    231\n",
       "1.0      0.763833  566    741"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.groupby(['hasWlan'])['noFirstLoanDefault'].agg(['mean', 'sum', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_employed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>0.824645</td>\n",
       "      <td>174</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unknown</th>\n",
       "      <td>0.554386</td>\n",
       "      <td>158</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>0.819328</td>\n",
       "      <td>390</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean  sum  count\n",
       "is_employed                      \n",
       "no           0.824645  174    211\n",
       "unknown      0.554386  158    285\n",
       "yes          0.819328  390    476"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.groupby(['is_employed'])['noFirstLoanDefault'].agg(['mean', 'sum', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>college or above</th>\n",
       "      <td>0.820755</td>\n",
       "      <td>435</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high_school</th>\n",
       "      <td>0.796791</td>\n",
       "      <td>149</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none/vacant</th>\n",
       "      <td>0.506438</td>\n",
       "      <td>118</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      mean  sum  count\n",
       "education                             \n",
       "college or above  0.820755  435    530\n",
       "high_school       0.796791  149    187\n",
       "low               0.909091   20     22\n",
       "none/vacant       0.506438  118    233"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.groupby(['education'])['noFirstLoanDefault'].agg(['mean', 'sum', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "# we can use a quick random forest to get the variable importance to once again support the logistic regression results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Understand the customers through dat_mpesa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrate the dat with the dat_mpesa_id to understand the behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(832, 37)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge dat and dat_mepsa_id\n",
    "dat_repay_mpesa_id = pd.merge(dat, dat_mpesa_id, left_index = True, right_index = True, how = 'left')\n",
    "# consider missing as random, delete them\n",
    "dat_repay_mpesa_id.drop(dat_repay_mpesa_id.index[dat_repay_mpesa_id.isnull().sum(axis=1)>3], \n",
    "                        inplace = True)\n",
    "dat_repay_mpesa_id.shape # (832, 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noFirstLoanDefault</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166</td>\n",
       "      <td>18.445783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>666</td>\n",
       "      <td>25.948949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count       mean\n",
       "noFirstLoanDefault                  \n",
       "0                     166  18.445783\n",
       "1                     666  25.948949"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the transaction number difference by repayment\n",
    "dat_repay_mpesa_id.groupby(['noFirstLoanDefault'])['count'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finding 1: Customers that repay first loans on average have more transactions and more active than customers that do not repay first loans__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noFirstLoanDefault</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166</td>\n",
       "      <td>0.039283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>666</td>\n",
       "      <td>0.025180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count      mean\n",
       "noFirstLoanDefault                 \n",
       "0                     166  0.039283\n",
       "1                     666  0.025180"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the net_count\n",
    "dat_repay_mpesa_id.groupby(['noFirstLoanDefault'])['net_count'].agg(['count', 'mean']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finding 2: Customers that repay first loans on average have more in-transactions than customers that do not repay first loans__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>avg_amt_out</th>\n",
       "      <th>avg_amt_in</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noFirstLoanDefault</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1902.099023</td>\n",
       "      <td>2411.226442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2522.464387</td>\n",
       "      <td>3477.789643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    avg_amt_out   avg_amt_in\n",
       "                           mean         mean\n",
       "noFirstLoanDefault                          \n",
       "0                   1902.099023  2411.226442\n",
       "1                   2522.464387  3477.789643"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the net_count\n",
    "dat_repay_mpesa_id.groupby(['noFirstLoanDefault'])['avg_amt_out', 'avg_amt_in'].agg(['mean']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finding 3: Customers that repay first loans on average have larger transaction amount than customers that do not repay first loans__.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Modeling the Life-Time-Value (LTV) of Borrowers\n",
    "\n",
    "As we discussed in part 2, analyzing the likelihood of first loan repayment doesn't equal to analyzing the overall repayment outcomes. Paying back the first loan doesn't mean the borrower is a valuable customer.  \n",
    "<br>\n",
    "Here, we want to construct a complex metrics called __life-time-value (LTV)__. Usually the __LTV__ should be a discrete number but as the dataset is small, defining a discrete LTV would introduce a lot of variance. So as a workaround, we create a __binary LTV__, just identifying whether the borrower is valuable or not given their past loan history.  \n",
    "<br>\n",
    "As we are __creating a new dependent variable (LTV)__, we need to label each record with the corresponding value. Normally this is realized through some experiential __rule-based labeling__ and __manually determining the class of complicated records near the decision boundary__. Here we use the __rule-based labeling__ method to determine the target dependent variable. This approach is bit arbitrary and subjective, but definitely would be useful as a __Proof-of-Concept (POC)__ and improved later for productionalized labeling. \n",
    "<br> \n",
    "\n",
    "Recall our value proposition in the beginning of the report. We define the profile of three kind of customers based on their __loan frequency__ and __number of historial loans__:\n",
    "\n",
    "* __High LTV Customers__: meet at least one of the following,\n",
    "    * frequent loans in a period/ periods based on a significant number of loans;\n",
    "    * large number of loans in history;\n",
    "* __Low LTV  Customers__: \n",
    "    * Default in first a few loans, for example, within first 3 loans;\n",
    "* __Unclear LTV Customers__: (to be removed)\n",
    "    * New customers with no default but just quite a small number of loans, for example, no more than 3 loans;   \n",
    "\n",
    "\n",
    "We create one more field __LTV__ based on the following rules:\n",
    "* __Rule 1__: if a customer defaults within the first 3 loans, we label he/she as Low LTV Customer;\n",
    "* __Rule 2__: if a customer never defaults but has no more than 3 loans, remove the record since LTV is unclear;  We assume these borrowers are randomly distributed and dropping them does not influence the distribution.  \n",
    "* __Rule 3__: if the median of loan intervals of a customer is within 5 weeks (35 days), label he/she as High LTV Customer ; \n",
    "* __Rule 4__: if a customer has over 10 historical loans, label he/she as High LTV Customer, no matter he/she eventually defaults or not. Even if they do default, the total return is almost positive and we should think about ways to retain these customers as they might churn because of not finding a more customized product/loan plan.  \n",
    "\n",
    "Approach: analyze statistics in logistic regression result   \n",
    "Target: fit a model that predict the LTV   \n",
    "\n",
    "__Note__: Those are not the optimal rules, but it labels the customers in a way which is decent enough to empower predictions. But given the small data size and subjective labeling philosophy, we should not try to optimize the prediciton results or interpret too much. Combined with some expertise checking, the rule could be more complex and robust given more dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Prepare the dataset: dat_ltv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy to start, as we will drop some records\n",
    "dat_ltv = dat.copy()\n",
    "# dat_ltv[['LTV', 'noFirstLoanDefault']].corr() -> corr = 0.62, not very correlated\n",
    "# Conduct Rule 2:\n",
    "dat_ltv.drop(dat_ltv.index[((dat_ltv['nLoans']<=3)&(dat_ltv['anyDefault']==0))], inplace = True) # Rule 2\n",
    "# get the median loan intervals\n",
    "# assign all within3loansDefault=1 with a extremely high value 999 (unit: months)\n",
    "dat_ltv['freq'] = 999 \n",
    "def get_time_intervals(date_string):\n",
    "    \"\"\"\n",
    "    type: string\n",
    "    rtype: float\n",
    "    \n",
    "    example:\n",
    "    input: date_string = '2018-07-01;2018-07-03;2018-07-05'\n",
    "    output: 2.0 (unit: day)\n",
    "    \"\"\"\n",
    "    date_list = date_string.split(';')\n",
    "    if len(date_list) > 1:\n",
    "        return(statistics.median([(pd.to_datetime(date_list[i+1]) - pd.to_datetime(date_list[i])).days \n",
    "                                  for i in range(len(date_list)-1)]))\n",
    "    else: \n",
    "        raise ValueError('Input only contains 1 string')\n",
    "# calculate the median loan intervals of records with within3loansDefault=0\n",
    "dat_ltv['freq'] = dat_ltv.apply(lambda row: get_time_intervals(row['loanStartDates']) \n",
    "                                if row['within3loansDefault']==0 \n",
    "                                else row['freq'], \n",
    "                                axis = 1)\n",
    "# Rule 1 & 3:\n",
    "# 35 days is an arbitrary number in Rule 3\n",
    "dat_ltv['LTV'] = dat_ltv['freq'].apply(lambda x: 1 if x<35 else 0)\n",
    "# Rule 4:\n",
    "dat_ltv.loc[dat_ltv['nLoans']>=10, 'LTV'] = 1 # here actually no change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Modeling the LTV using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.647842\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>       <td>No. Iterations:</td>   <td>5.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>        <td>LTV</td>       <td>Pseudo R-squared:</td>   <td>0.054</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2018-07-17 00:00</td>       <td>AIC:</td>        <td>995.2851</td> \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>745</td>             <td>BIC:</td>        <td>1064.4859</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>14</td>         <td>Log-Likelihood:</td>   <td>-482.64</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>730</td>           <td>LL-Null:</td>       <td>-510.06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>        <td>1.0000</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                  <td></td>                  <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>how_much_paid</th>                    <td>0.2702</td>   <td>0.0870</td>  <td>3.1037</td>  <td>0.0019</td> <td>0.0996</td>  <td>0.4407</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>                              <td>0.0927</td>   <td>0.0778</td>  <td>1.1915</td>  <td>0.2335</td> <td>-0.0598</td> <td>0.2452</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hasWlan</th>                          <td>0.0340</td>   <td>0.1823</td>  <td>0.1864</td>  <td>0.8521</td> <td>-0.3233</td> <td>0.3912</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender_F</th>                         <td>0.3184</td>   <td>0.1776</td>  <td>1.7922</td>  <td>0.0731</td> <td>-0.0298</td> <td>0.6665</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>living_situation_family&stdhouse</th> <td>0.2785</td>   <td>0.3285</td>  <td>0.8479</td>  <td>0.3965</td> <td>-0.3653</td> <td>0.9224</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>living_situation_other</th>           <td>-0.0106</td>  <td>0.4441</td>  <td>-0.0240</td> <td>0.9809</td> <td>-0.8811</td> <td>0.8598</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>living_situation_own_home</th>        <td>0.1099</td>   <td>0.2095</td>  <td>0.5245</td>  <td>0.5999</td> <td>-0.3008</td> <td>0.5206</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_high_school</th>            <td>0.0050</td>   <td>0.2090</td>  <td>0.0238</td>  <td>0.9810</td> <td>-0.4046</td> <td>0.4145</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_low</th>                    <td>0.2237</td>   <td>0.5818</td>  <td>0.3846</td>  <td>0.7005</td> <td>-0.9165</td> <td>1.3640</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_none/vacant</th>            <td>-1.0440</td>  <td>0.3401</td>  <td>-3.0698</td> <td>0.0021</td> <td>-1.7106</td> <td>-0.3775</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>is_employed_no</th>                   <td>-0.5647</td>  <td>0.3386</td>  <td>-1.6676</td> <td>0.0954</td> <td>-1.2285</td> <td>0.0990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>is_employed_yes</th>                  <td>-0.4327</td>  <td>0.3119</td>  <td>-1.3874</td> <td>0.1653</td> <td>-1.0439</td> <td>0.1786</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>andrVer_advanced</th>                 <td>1.1364</td>   <td>0.6637</td>  <td>1.7122</td>  <td>0.0869</td> <td>-0.1644</td> <td>2.4372</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>andrVer_medium</th>                   <td>0.5217</td>   <td>0.2227</td>  <td>2.3423</td>  <td>0.0192</td> <td>0.0852</td>  <td>0.9583</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>                        <td>-0.2602</td>  <td>0.3788</td>  <td>-0.6868</td> <td>0.4922</td> <td>-1.0026</td> <td>0.4823</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                                 Results: Logit\n",
       "================================================================================\n",
       "Model:                    Logit                 No. Iterations:        5.0000   \n",
       "Dependent Variable:       LTV                   Pseudo R-squared:      0.054    \n",
       "Date:                     2018-07-17 00:00      AIC:                   995.2851 \n",
       "No. Observations:         745                   BIC:                   1064.4859\n",
       "Df Model:                 14                    Log-Likelihood:        -482.64  \n",
       "Df Residuals:             730                   LL-Null:               -510.06  \n",
       "Converged:                1.0000                Scale:                 1.0000   \n",
       "--------------------------------------------------------------------------------\n",
       "                                  Coef.  Std.Err.    z    P>|z|   [0.025  0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "how_much_paid                     0.2702   0.0870  3.1037 0.0019  0.0996  0.4407\n",
       "age                               0.0927   0.0778  1.1915 0.2335 -0.0598  0.2452\n",
       "hasWlan                           0.0340   0.1823  0.1864 0.8521 -0.3233  0.3912\n",
       "gender_F                          0.3184   0.1776  1.7922 0.0731 -0.0298  0.6665\n",
       "living_situation_family&stdhouse  0.2785   0.3285  0.8479 0.3965 -0.3653  0.9224\n",
       "living_situation_other           -0.0106   0.4441 -0.0240 0.9809 -0.8811  0.8598\n",
       "living_situation_own_home         0.1099   0.2095  0.5245 0.5999 -0.3008  0.5206\n",
       "education_high_school             0.0050   0.2090  0.0238 0.9810 -0.4046  0.4145\n",
       "education_low                     0.2237   0.5818  0.3846 0.7005 -0.9165  1.3640\n",
       "education_none/vacant            -1.0440   0.3401 -3.0698 0.0021 -1.7106 -0.3775\n",
       "is_employed_no                   -0.5647   0.3386 -1.6676 0.0954 -1.2285  0.0990\n",
       "is_employed_yes                  -0.4327   0.3119 -1.3874 0.1653 -1.0439  0.1786\n",
       "andrVer_advanced                  1.1364   0.6637  1.7122 0.0869 -0.1644  2.4372\n",
       "andrVer_medium                    0.5217   0.2227  2.3423 0.0192  0.0852  0.9583\n",
       "intercept                        -0.2602   0.3788 -0.6868 0.4922 -1.0026  0.4823\n",
       "================================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do the logistic regression once again\n",
    "var = ['gender', # binary ['M', 'F']\n",
    "       'living_situation', # 4 levels ['other', 'pay_rent', 'family&stdhouse', 'own_home']\n",
    "       'education', # 4 levels ['none/vacant', 'college or above', 'high_school', 'low']\n",
    "       'how_much_paid', # numeric\n",
    "       'is_employed', # 3 levels ['unknown', 'yes', 'no']\n",
    "       'age', # numeric\n",
    "       'hasWlan', # binary \n",
    "       'andrVer' # 3 levels ['old', 'medium', 'advanced']\n",
    "       ]\n",
    "\n",
    "# dummy the var if applicable \n",
    "dat_ltv_model = dat_ltv[var].copy()\n",
    "dat_ltv_model = pd.get_dummies(dat_ltv_model, drop_first=False)\n",
    "# set the base of each categorical variables by dropping them \n",
    "drop_var_list = ['gender_M', \n",
    "                 'living_situation_pay_rent',\n",
    "                 'education_college or above',\n",
    "                 'andrVer_old',\n",
    "                 'is_employed_unknown'\n",
    "                 ]\n",
    "dat_ltv_model.drop(drop_var_list, axis = 1, inplace = True)\n",
    "dat_ltv_model.shape # (745, 14)\n",
    "# standardize how_much_paid\n",
    "dat_ltv_model['how_much_paid'] = standardize(dat_ltv_model['how_much_paid'].apply(lambda x: math.log(x+1)))\n",
    "# standardize age\n",
    "dat_ltv_model['age'] = standardize(dat_ltv_model['age'])\n",
    "# append the target column: LTV\n",
    "dat_ltv_model['LTV'] = dat_ltv['LTV']\n",
    "\n",
    "\n",
    "# check correlation\n",
    "ltv_corr_matrix = dat_ltv_model.corr()  # decent, good enough\n",
    "\n",
    "# append intercept as 1\n",
    "dat_ltv_model['intercept'] = 1   \n",
    "# run a logistic regression\n",
    "model_ltv = sm.Logit(dat_ltv_model['LTV'], dat_ltv_model.loc[:, dat_ltv_model.columns != 'LTV'])\n",
    "result_ltv = model_ltv.fit(maxiter=100)\n",
    "result_ltv.summary2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that **is_employed_unknown** (1-is_employed_no-is_employed_yes) is highly correlated (corr=0.85) with **education_none/vacant**, so there could be multicollinearity in this problem. we need to iterate to do feature selection and make sure **is_employed_no**, **is_employed_yes** and **education_none/vacant** will not all appear in a same result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.651334\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>       <td>No. Iterations:</td>   <td>5.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>        <td>LTV</td>       <td>Pseudo R-squared:</td>   <td>0.049</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2018-07-17 00:00</td>       <td>AIC:</td>        <td>982.4875</td> \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>745</td>             <td>BIC:</td>        <td>1010.1678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>5</td>         <td>Log-Likelihood:</td>   <td>-485.24</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>739</td>           <td>LL-Null:</td>       <td>-510.06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>        <td>1.0000</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>             <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>how_much_paid</th>         <td>0.2793</td>   <td>0.0815</td>  <td>3.4257</td>  <td>0.0006</td> <td>0.1195</td>  <td>0.4391</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender_F</th>              <td>0.3028</td>   <td>0.1739</td>  <td>1.7416</td>  <td>0.0816</td> <td>-0.0380</td> <td>0.6436</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_none/vacant</th> <td>-0.6414</td>  <td>0.1902</td>  <td>-3.3726</td> <td>0.0007</td> <td>-1.0142</td> <td>-0.2687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>andrVer_advanced</th>      <td>1.1954</td>   <td>0.6560</td>  <td>1.8224</td>  <td>0.0684</td> <td>-0.0902</td> <td>2.4810</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>andrVer_medium</th>        <td>0.5124</td>   <td>0.2202</td>  <td>2.3272</td>  <td>0.0200</td> <td>0.0809</td>  <td>0.9440</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>             <td>-0.6035</td>  <td>0.2229</td>  <td>-2.7079</td> <td>0.0068</td> <td>-1.0403</td> <td>-0.1667</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                           Results: Logit\n",
       "=====================================================================\n",
       "Model:                 Logit             No. Iterations:    5.0000   \n",
       "Dependent Variable:    LTV               Pseudo R-squared:  0.049    \n",
       "Date:                  2018-07-17 00:00  AIC:               982.4875 \n",
       "No. Observations:      745               BIC:               1010.1678\n",
       "Df Model:              5                 Log-Likelihood:    -485.24  \n",
       "Df Residuals:          739               LL-Null:           -510.06  \n",
       "Converged:             1.0000            Scale:             1.0000   \n",
       "---------------------------------------------------------------------\n",
       "                       Coef.  Std.Err.    z    P>|z|   [0.025  0.975]\n",
       "---------------------------------------------------------------------\n",
       "how_much_paid          0.2793   0.0815  3.4257 0.0006  0.1195  0.4391\n",
       "gender_F               0.3028   0.1739  1.7416 0.0816 -0.0380  0.6436\n",
       "education_none/vacant -0.6414   0.1902 -3.3726 0.0007 -1.0142 -0.2687\n",
       "andrVer_advanced       1.1954   0.6560  1.8224 0.0684 -0.0902  2.4810\n",
       "andrVer_medium         0.5124   0.2202  2.3272 0.0200  0.0809  0.9440\n",
       "intercept             -0.6035   0.2229 -2.7079 0.0068 -1.0403 -0.1667\n",
       "=====================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional feature selection\n",
    "# we drop variable with the highest p-value each time until every variable is significant\n",
    "# This further reduce the overfitting and improve generalization power\n",
    "# run a logistic regression\n",
    "model_ltv = sm.Logit(dat_ltv_model['LTV'], \n",
    "                 dat_ltv_model.loc[:, ~(dat_ltv_model.columns.isin(['LTV',\n",
    "                                                                    'education_high_school',\n",
    "                                                                    'living_situation_other',\n",
    "                                                                    'hasWlan',\n",
    "                                                                    #'education_none/vacant',\n",
    "                                                                    'is_employed_yes',\n",
    "                                                                    'is_employed_no',\n",
    "                                                                    'living_situation_family&stdhouse',\n",
    "                                                                    'living_situation_own_home',\n",
    "                                                                    'education_low',\n",
    "                                                                   'age']))])\n",
    "result_ltv = model_ltv.fit(maxiter=100)\n",
    "result_ltv.summary2() # the result meets expectations\n",
    "# we don't exclude living_situation_other as it explains a potential direction of user segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Interpret this summary:__  \n",
    "    * __how much paid__, has a P-value of 0.0006 and a positive coef. It means the higher the customer gets paid, the more likely he/she is a High LTV Customer.   \n",
    "    * __gender female__, has a P-value of 0.08 and a postive coef. It means the female customers are more likely to be a High LTV Customer. Though the p-value is not ideally small to conclude with more confidence, it still provides a good direction that it is suggested to target female audience rather than male audience.   \n",
    "    * __education none or vacant__, has a p-value of 0.0007 and a large negative coef. It means people don't fill this field in the application are significantly less likely to be a High LTV Customer. \n",
    "    * As __is employed unknown__ is highly correlated with __education none or vacant__, this also suggest that people don't fill is_employed in the application are significantly less likely to be a High LTV Customer. \n",
    "    * __andriod version__, both 'advanced' and 'medium' have a relatively small p-values and positive coefs. We can see that advanced andriod version has a large coef and while medium andriod version has a relative smaller one. Since the base is the old andriod version, this simply means the higher andriod version, the more likely of being High LTV Customers.      \n",
    "    * __age__, __hasWlan__, and __living_situation__ do not show a statistically significant impact on the likelihood of being a High LTV Customer.  \n",
    "    \n",
    "Repeat what has been discussed in part 2:   \n",
    "\n",
    "In the real world, __making decisions through blur observations__ is very important. Though not very significant statistically, for exmaple, p-value slightly larger than 0.05, it is still useful to have a sense of how to make the decision if there has to be one.   \n",
    "    \n",
    "As the dataset contains 754 records, which is a relatively high record-variable ratio for logisic regression. Overfitting is not a major concern here. So we don't partition the data into training and test. Also, we care more about the coef of each variable rather than whether the model achieves its best predictive power. Simply fitting all the dataset with logistic regression would be enough to generalize the direction of the variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Support the decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0.472081</td>\n",
       "      <td>93</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.421533</td>\n",
       "      <td>231</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean  sum  count\n",
       "gender                      \n",
       "F       0.472081   93    197\n",
       "M       0.421533  231    548"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_ltv.groupby(['gender'])['LTV'].agg(['mean', 'sum', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>college or above</th>\n",
       "      <td>0.505348</td>\n",
       "      <td>189</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high_school</th>\n",
       "      <td>0.467626</td>\n",
       "      <td>65</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>0.538462</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none/vacant</th>\n",
       "      <td>0.287671</td>\n",
       "      <td>63</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      mean  sum  count\n",
       "education                             \n",
       "college or above  0.505348  189    374\n",
       "high_school       0.467626   65    139\n",
       "low               0.538462    7     13\n",
       "none/vacant       0.287671   63    219"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_ltv.groupby(['education'])['LTV'].agg(['mean', 'sum', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_employed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>0.434483</td>\n",
       "      <td>63</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unknown</th>\n",
       "      <td>0.345725</td>\n",
       "      <td>93</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>0.507553</td>\n",
       "      <td>168</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean  sum  count\n",
       "is_employed                      \n",
       "no           0.434483   63    145\n",
       "unknown      0.345725   93    269\n",
       "yes          0.507553  168    331"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_ltv.groupby(['is_employed'])['LTV'].agg(['mean', 'sum', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>andrVer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>advanced</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>0.468484</td>\n",
       "      <td>275</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <td>0.280822</td>\n",
       "      <td>41</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mean  sum  count\n",
       "andrVer                       \n",
       "advanced  0.666667    8     12\n",
       "medium    0.468484  275    587\n",
       "old       0.280822   41    146"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_ltv.groupby(['andrVer'])['LTV'].agg(['mean', 'sum', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Understand the customers through dat_mpesa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrate the dat with the dat_mpesa_id to understand the behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(631, 39)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge dat_ltv and dat_mepsa_id\n",
    "dat_ltv_mpesa_id = pd.merge(dat_ltv, dat_mpesa_id, left_index = True, right_index = True, how = 'left')\n",
    "# consider missing as random, delete them\n",
    "dat_ltv_mpesa_id.drop(dat_ltv_mpesa_id.index[dat_ltv_mpesa_id.isnull().sum(axis=1)>3], \n",
    "                        inplace = True)\n",
    "dat_ltv_mpesa_id.shape # (631, 39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LTV</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321</td>\n",
       "      <td>21.383178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>310</td>\n",
       "      <td>27.816129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count       mean\n",
       "LTV                  \n",
       "0      321  21.383178\n",
       "1      310  27.816129"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the transaction number difference by repayment\n",
    "dat_ltv_mpesa_id.groupby(['LTV'])['count'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finding 1: Customers that have high LTV on average have more transactions and more active than customers that do not have high LTV__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LTV</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321</td>\n",
       "      <td>0.032441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>310</td>\n",
       "      <td>0.023643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count      mean\n",
       "LTV                 \n",
       "0      321  0.032441\n",
       "1      310  0.023643"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the net_count\n",
    "dat_ltv_mpesa_id.groupby(['LTV'])['net_count'].agg(['count', 'mean']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finding 2: Customers that have high LTV on average have more in-transactions than customers that do not have high LTV__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>avg_amt_out</th>\n",
       "      <th>avg_amt_in</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LTV</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2068.729972</td>\n",
       "      <td>2638.526731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2747.704328</td>\n",
       "      <td>3973.168816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     avg_amt_out   avg_amt_in\n",
       "            mean         mean\n",
       "LTV                          \n",
       "0    2068.729972  2638.526731\n",
       "1    2747.704328  3973.168816"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the net_count\n",
    "dat_ltv_mpesa_id.groupby(['LTV'])['avg_amt_out', 'avg_amt_in'].agg(['mean']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finding 3: Customers that have high LTV on average have larger transaction amount than customers that do not have high LTV__.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary of Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Recommendation - User Acquisition\n",
    "* According to model results in analyzing LTV and Repayment, marketing and user acquisition on Facebook should focus on: <br>\n",
    "    * Devices with higher android version; <br>\n",
    "    * Female customers; <br>\n",
    "    * Customers with higher incomes; <br>\n",
    "    * Customers with clear employment status and education history; <br>\n",
    "<br>\n",
    "* Build referral program: <br>\n",
    "    * If a new customer was referred by any source, he/she is very likely (83%) to repay the first loan; <br>\n",
    "<br>\n",
    "* Target customers with more frequent financial activities: <br>\n",
    "    * Using Mpesa data, we identify that on average high LTV customers transact more frequently; <br>\n",
    "<br>\n",
    "* Target customers with stable relationship status: <br>\n",
    "    * Married people have better repayment as they are more likely to have stable financial condition; <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Recommendation - Loan Application & Product \n",
    "* Set up some simple rules during the loan application process; <br>\n",
    "    * For example, the credit system should reject the loan application if it fails to collect enough device information.  <br>\n",
    "<br>\n",
    "* Build out more flexible and customized loan plans (products) for customers; <br>\n",
    "    * Customers with a few historical loans demonstrate their need of loans; <br>\n",
    "    * They churn (with a default or not), potentially looking for other peer loan products on the market; <br>\n",
    "    * Potentially useful products to help with user retention include (not limited to): <br>\n",
    "        * Set up a credit line (with a slightly higher interest rate), instead of repaying all the amount before making a new loan; it helps retain high LTV customers as they transact more frequently; <br>\n",
    "        * Add more plans in addition to ‘three weekly repayments’; for example, one monthly repayment would be more appealing as there might be a seasonality effect that customers need loans to pay for monthly bills like house-rent according to ‘signup_date’ field in Data Quality Report (DQR); <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for reading\n",
    "### Please reach out to me via cheng.chen.2017@marshall.usc.edu for more details or discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
